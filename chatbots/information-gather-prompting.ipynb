{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa8f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = os.getenv(\"LANGSMITH_TRACING\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80623650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model = \"gpt-oss:20b\",\n",
    "    validate_model_on_init = True,\n",
    "    base_url = \"http://192.168.1.240:11434\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f6b6cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import SystemMessage\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "343a873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Your job is to get information from a user about what type of prompt template they want to create.\n",
    "\n",
    "You should get the following information from them:\n",
    "\n",
    "- What the objective of the prompt is\n",
    "- What variables will be passed into the prompt template\n",
    "- Any constraints for what the output should NOT do\n",
    "- Any requirements that the output MUST adhere to\n",
    "\n",
    "If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "\n",
    "After you are able to discern all the information, call the relevant tool.\"\"\"\n",
    "\n",
    "\n",
    "def get_messages_info(messages):\n",
    "    return [SystemMessage(content=template)] + messages\n",
    "\n",
    "\n",
    "class PromptInstructions(BaseModel):\n",
    "    \"\"\"Instructions on how to prompt the LLM.\"\"\"\n",
    "\n",
    "    objective: str\n",
    "    variables: List[str]\n",
    "    constraints: List[str]\n",
    "    requirements: List[str]\n",
    "\n",
    "\n",
    "llm_with_tool = llm.bind_tools([PromptInstructions])\n",
    "\n",
    "\n",
    "def info_chain(state):\n",
    "    messages = get_messages_info(state[\"messages\"])\n",
    "    response = llm_with_tool.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74dc3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "\n",
    "# New system prompt\n",
    "prompt_system = \"\"\"Based on the following requirements, write a good prompt template:\n",
    "\n",
    "{reqs}\"\"\"\n",
    "\n",
    "\n",
    "# Function to get the messages for the prompt\n",
    "# Will only get messages AFTER the tool call\n",
    "def get_prompt_messages(messages: list):\n",
    "    tool_call = None\n",
    "    other_msgs = []\n",
    "    for m in messages:\n",
    "        if isinstance(m, AIMessage) and m.tool_calls:\n",
    "            tool_call = m.tool_calls[0][\"args\"]\n",
    "        elif isinstance(m, ToolMessage):\n",
    "            continue\n",
    "        elif tool_call is not None:\n",
    "            other_msgs.append(m)\n",
    "    return [SystemMessage(content=prompt_system.format(reqs=tool_call))] + other_msgs\n",
    "\n",
    "\n",
    "def prompt_gen_chain(state):\n",
    "    messages = get_prompt_messages(state[\"messages\"])\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba41b6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "def get_state(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:\n",
    "        return \"add_tool_message\"\n",
    "    elif not isinstance(messages[-1], HumanMessage):\n",
    "        return END\n",
    "    return \"info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09791e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "memory = InMemorySaver()\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"info\", info_chain)\n",
    "workflow.add_node(\"prompt\", prompt_gen_chain)\n",
    "\n",
    "\n",
    "@workflow.add_node\n",
    "def add_tool_message(state: State):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=\"Prompt generated!\",\n",
    "                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\"info\", get_state, [\"add_tool_message\", \"info\", END])\n",
    "workflow.add_edge(\"add_tool_message\", \"prompt\")\n",
    "workflow.add_edge(\"prompt\", END)\n",
    "workflow.add_edge(START, \"info\")\n",
    "graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "858e060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit): \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  PromptInstructions (7d8f9e67-72e4-413a-b8a8-fa513e20225d)\n",
      " Call ID: 7d8f9e67-72e4-413a-b8a8-fa513e20225d\n",
      "  Args:\n",
      "    constraints: []\n",
      "    objective: \n",
      "    requirements: []\n",
      "    variables: []\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Prompt generated!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Below is a **generic prompt‑template** that you can copy‑and‑paste and fill in with the details that apply to your specific use‑case.  \n",
      "All the fields surrounded by **square brackets** are meant to be replaced by the user; keep them as placeholders so you can re‑use the template for any new problem.\n",
      "\n",
      "```text\n",
      "────────────────────────────────────────────────────────────────────────────\n",
      "**Prompt Template**\n",
      "\n",
      "You are a **[ROLE]**.  \n",
      "Your **objective** is to **[OBJECTIVE]**.  \n",
      "\n",
      "**Constraints** (if any, otherwise write \"None\"):\n",
      "- [CONSTRAINT 1]\n",
      "- [CONSTRAINT 2]\n",
      "- … \n",
      "\n",
      "**Requirements** (must‑haves for the final answer, if any):\n",
      "- [REQUIREMENT 1]\n",
      "- [REQUIREMENT 2]\n",
      "- … \n",
      "\n",
      "**Variables** you may reference (provide a brief description for each):\n",
      "- **[VAR1]** – [description]\n",
      "- **[VAR2]** – [description]\n",
      "- … \n",
      "\n",
      "**Desired Output Format** (e.g., bullet list, JSON, code snippet, etc.):\n",
      "- [OUTPUT FORMAT DESCRIPTION]\n",
      "\n",
      "**Example** (Optional – show a sample of what you expect as output):\n",
      "```json\n",
      "{\n",
      "  \"result\": \"...\",\n",
      "  \"details\": \"...\"\n",
      "}\n",
      "```\n",
      "────────────────────────────────────────────────────────────────────────────\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### How to Use This Template\n",
      "\n",
      "1. **Replace** every bracketed placeholder with your own content.  \n",
      "   *If a section does not apply, you can simply delete that bullet point or write “None”.*\n",
      "\n",
      "2. **Keep the structure**; it helps the AI understand the hierarchy of the task: role → objective → constraints → requirements → variables → output format.\n",
      "\n",
      "3. **Add any extra instructions** (e.g., tone, word limit, specific style) at the end of the prompt if needed.\n",
      "\n",
      "4. **Copy the fully‑filled prompt** into the chat and submit. The AI will then generate an answer that respects all the constraints, meets the requirements, and uses the supplied variables.\n",
      "\n",
      "---\n",
      "\n",
      "Feel free to tweak the wording or add more sections (e.g., “Assumptions”, “Time constraints”) if your tasks call for it. Happy prompting!\n",
      "Done!\n",
      "User (q/Q to quit):     objective: rag     variables: ['none']     constraints: ['no']     requirements: ['no']\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  PromptInstructions (ca871086-6dbb-47d7-9c82-cdb66dddc1e1)\n",
      " Call ID: ca871086-6dbb-47d7-9c82-cdb66dddc1e1\n",
      "  Args:\n",
      "    constraints: []\n",
      "    objective: rag\n",
      "    requirements: []\n",
      "    variables: []\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Prompt generated!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**RAG (Retrieval‑Augmented Generation) Prompt Template**\n",
      "\n",
      "> **Role**  \n",
      "> You are a *Retrieval‑Augmented Generation (RAG) assistant* that combines search and natural‑language generation.\n",
      "\n",
      "> **Objective**  \n",
      "> Generate a concise, accurate response to the user’s query by retrieving the most relevant documents from the knowledge base and synthesizing an answer from them.\n",
      "\n",
      "> **Constraints**  \n",
      "> *None*  \n",
      "\n",
      "> **Requirements**  \n",
      "> *None*  \n",
      "\n",
      "> **Variables**  \n",
      "> *None*  \n",
      "\n",
      "> **Prompt Structure**  \n",
      "> 1. **User Query** – The question or request that the user has.  \n",
      "> 2. **Retrieval Step** – Identify the top‑k most relevant documents (or passages) from the knowledge base.  \n",
      "> 3. **Generation Step** – Using the retrieved evidence, produce a final answer that is:\n",
      ">    * Factually correct\n",
      ">    * Self‑contained (does not require the user to consult the source documents)\n",
      ">    * Clearly cites the source (e.g., “According to [Doc #1]…”)  \n",
      "> 4. **Output Format** – Return the answer in plain text (or JSON if preferred).\n",
      "\n",
      "> **Sample Prompt**  \n",
      "> ```text\n",
      "> **User Query:**  \n",
      "> \"What are the main causes of the 2019–2020 global recession?\"\n",
      "> \n",
      "> **Retrieval Instructions:**  \n",
      "> 1. Search the knowledge base for the 5 most relevant documents or passages about the 2019–2020 global recession causes.  \n",
      "> 2. List each source with its unique ID and a short excerpt that supports the answer.\n",
      "> \n",
      "> **Generation Instructions:**  \n",
      "> 1. Read the retrieved sources.  \n",
      "> 2. Compose a concise answer that cites the sources (e.g., “Source #2: …”).  \n",
      "> 3. Keep the answer <200 words.\n",
      "> \n",
      "> **Answer (plain text):**  \n",
      "> (The assistant writes the answer here)\n",
      "> ```\n",
      "\n",
      "> **Usage Tips**  \n",
      "> * Replace “User Query” with the actual user question.  \n",
      "> * If you want the answer in a specific format (JSON, Markdown, etc.), state it explicitly in the “Output Format” section.  \n",
      "> * For advanced RAG setups, you can add additional variables such as “knowledge base ID”, “retrieval API key”, or “confidence threshold” and incorporate them into the retrieval step.  \n",
      "> * When no constraints or requirements are specified, keep the prompt as minimal as possible—just the role, objective, and instruction to retrieve‑and‑generate.  \n",
      "\n",
      "Feel free to copy this template into your RAG pipeline and adjust the placeholders as needed!\n",
      "Done!\n",
      "User (q/Q to quit): q\n",
      "AI: Byebye\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "cached_human_responses = [\"hi!\", \"rag prompt\", \"1 rag, 2 none, 3 no, 4 no\", \"red\", \"q\"]\n",
    "cached_response_index = 0\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "while True:\n",
    "    try:\n",
    "        user = input(\"User (q/Q to quit): \")\n",
    "    except:\n",
    "        user = cached_human_responses[cached_response_index]\n",
    "        cached_response_index += 1\n",
    "    print(f\"User (q/Q to quit): {user}\")\n",
    "    if user in {\"q\", \"Q\"}:\n",
    "        print(\"AI: Byebye\")\n",
    "        break\n",
    "    output = None\n",
    "    for output in graph.stream(\n",
    "        {\"messages\": [HumanMessage(content=user)]}, config=config, stream_mode=\"updates\"\n",
    "    ):\n",
    "        last_message = next(iter(output.values()))[\"messages\"][-1]\n",
    "        last_message.pretty_print()\n",
    "\n",
    "    if output and \"prompt\" in output:\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9c9a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
